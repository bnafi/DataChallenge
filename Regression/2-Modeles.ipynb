{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0 - Démarches préliminaires"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.a- Importation des librairies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
     ]
    }
   ],
   "source": [
    "#general librairies\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import yaml \n",
    "\n",
    "#sklearn librairies\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import *\n",
    "from sklearn.ensemble import * \n",
    "from sklearn.svm import *\n",
    "import statsmodels.api as sm\n",
    "import xgboost as xgb\n",
    "\n",
    "#appel a nos fonctions\n",
    "from fcts_R.general import * \n",
    "from fcts_R.dataset_division import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.b - On télécharge le jeu de données train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"paths_datasets_r.yaml\", \"r\") as yaml_file:\n",
    "    data = yaml.load(yaml_file, Loader=yaml.FullLoader)\n",
    "\n",
    "path_train = data[\"train\"]\n",
    "\n",
    "#On charge le jeu de données\n",
    "data = pd.read_csv(path_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans la suite, on va tenter de faire des prédictions sur chaque type de vin."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Traitement du jeu de données "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a- Séparation du jeu de données initial en fonction de 'wine_type'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#on extrait indices dans lequels la variable \"wine_type\"= 0 (resp. = 1)\n",
    "idx0, idx1 = winetype(data)\n",
    "\n",
    "#on crée les jeu de données data0 et data1 à partir des indices \n",
    "data0, data1 = formal_div(data, idx0, idx1)\n",
    "\n",
    "data0.to_csv(\"Datasets_R/data0.csv\")\n",
    "data1.to_csv(\"Datasets_R/data1.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b - Séparation train/test: \n",
    "Cette division nous permettra d'évaluer les performances de nos modèles en contrôlant l'erreur test. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data0 = pd.read_csv(\"Datasets_R/data0.csv\")\n",
    "data1 = pd.read_csv(\"Datasets_R/data1.csv\")\n",
    "\n",
    "\n",
    "#On sépare les co-variables de la variable à prédire\n",
    "X0,y0 = treatment(data0)\n",
    "X1,y1 = treatment(data1)\n",
    "\n",
    "#Data0: \n",
    "X_tr0, X_te0, y_tr0, y_te0= train_test_split(X0,y0, test_size=0.33,random_state=2023)\n",
    "\n",
    "#Data1:\n",
    "X_tr1, X_te1, y_tr1, y_te1= train_test_split(X1,y1, test_size=0.33,random_state=2023)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c - Normalisation des co-variables\n",
    "Cette démarche nous permettra d'éviter des biais dûs à d'unité des covariables de l'étude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pour data0: \n",
    "stdsc = StandardScaler()\n",
    "X_tr_0 = stdsc.fit_transform(X_tr0)\n",
    "X_te_0 = stdsc.transform(X_te0)\n",
    "\n",
    "#Pour data1: \n",
    "stdsc = StandardScaler()\n",
    "X_tr_1 = stdsc.fit_transform(X_tr1)\n",
    "X_te_1 = stdsc.transform(X_te1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Sélection et entrainement de modèles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a - Sélection de paramètres et modèles basée sur la cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1- SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modèle entrainé sur data0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters:  {'C': 1, 'gamma': 'scale', 'kernel': 'rbf', 'nu': 0.6}\n",
      "Best Score:  0.3692312928254857\n",
      "erreur test:  0.40566250675181714\n"
     ]
    }
   ],
   "source": [
    "# {'kernel': [\"linear\",\"poly\",\"rbf\", \"sigmoid\"], 'gamma':[\"scale\", \"auto\"], 'epsilon':[0.05,0.1,0.15], 'C': [0.5,1,1.5]}\n",
    "#Best Hyperparameters:  {'C': 1, 'epsilon': 0.15, 'gamma': 'scale', 'kernel': 'rbf'}; Best Score:  0.37465693229921293; normal:  0.40728418982541437 \n",
    "\n",
    "#{'degree': [1,2,3,4,5,6]}, SVR(kernel=\"poly\", gamma=\"scale\", epsilon=0.15)\n",
    "#Best Hyperparameters:  {'degree': 1}; Best Score:  0.26182202519203174; normal:  0.3233410341186739\n",
    "\n",
    "#{'degree': [1,2,3,4,5,6], 'coef0': [0.0, 0.01, 0.1, 0.5, 0.75,1]}, SVR(kernel=\"poly\", gamma=\"scale\", epsilon=0.15)\n",
    "#Best Hyperparameters:  {'coef0': 1, 'degree': 2}; Best Score:  0.31889755456529084; normal:  0.26912361991020595\n",
    "\n",
    "#{'C': [0.75,0.9,1,1.1,1.25]}, SVR(kernel=\"rbf\", gamma=\"scale\", epsilon=0.15)\n",
    "#Best Hyperparameters:  {'C': 1}; Best Score:  0.37465693229921293; normal:  0.40728418982541437\n",
    "\n",
    "#{'loss': [\"epsilon_insensitive\", \"squared_epsilon_insensitive\"], 'epsilon': [0,0.01,0.05,0.1,0.2,0.5],  'C': [0.5,1,1.5]},LinearSVR()\n",
    "# Best Hyperparameters:  {'C': 1.5, 'epsilon': 0, 'loss': 'squared_epsilon_insensitive'}; Best Score:  0.26364596987343536; normal:  0.3142987698900451\n",
    "\n",
    "#BEST SCORE: \n",
    "#params = {'kernel': [\"linear\",\"poly\",\"rbf\", \"sigmoid\"], 'gamma':[\"scale\", \"auto\"], 'nu':[0.25,0.4,0.5,0.6,0.75,1], 'C': [0.5,1,1.5]}, NuSVR()\n",
    "# Best Hyperparameters:  {'C': 1.5, 'gamma': 'scale', 'kernel': 'rbf', 'nu': 0.6}; Best Score:  0.3825024346626745; normal:  0.4170237776338953\n",
    "\n",
    "params = {'kernel': [\"linear\",\"poly\",\"rbf\", \"sigmoid\"], 'gamma':[\"scale\", \"auto\"], 'nu':[0.25,0.4,0.5,0.6,0.75,1], 'C': [0.5,1,1.5]}\n",
    "pred0 = param_selection(params, NuSVR(), X_tr_0, y_tr0.to_numpy().ravel(), X_te_0)\n",
    "\n",
    "print(\"erreur test: \", r2_score(y_te0, pred0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modèle entrainé sur data1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "erreur test:  0.3745598443683087\n"
     ]
    }
   ],
   "source": [
    "#pred_SVM1 = param_selection({'kernel': [\"linear\",\"poly\",\"rbf\", \"sigmoid\"], 'gamma':[\"scale\", \"auto\"], 'epsilon':[0.05,0.1,0.15], 'C': [0.5,1,1.5]}, SVR(gamma='auto'), X_tr_1,y_tr1, X_te_1)\n",
    "# Best Hyperparameters:  {'C': 0.5, 'epsilon': 0.15, 'gamma': 'scale', 'kernel': 'rbf'}; Best Score:  0.36646962843446423\n",
    "\n",
    "#pred_SVM1 = param_selection({'kernel': [\"linear\",\"poly\",\"rbf\"], 'epsilon':[0.01,0.1,0.2]}, SVR(gamma=\"auto\"), X_tr_1, y_tr1, X_te_1)\n",
    "# Best Hyperparameters:  {'epsilon': 0.2, 'kernel': 'rbf'}; Best Score:  0.37085380472521134\n",
    "\n",
    "#pred_SVM1 = param_selection({'degree': [1,2,3,4]}, SVR(gamma=\"auto\", kernel=\"poly\", epsilon=0.2), X_tr_1, y_tr1, X_te_1)\n",
    "#Best Hyperparameters:  {'degree': 1}; Best Score:  0.3486973341420462\n",
    "\n",
    "#pred_SVM1 = param_selection({'loss': [\"epsilon_insensitive\", \"squared_epsilon_insensitive\"], 'epsilon': [0,0.01,0.05,0.1,0.2,0.5],  'C': [0.5,1,1.5]},LinearSVR(), X_tr_1,y_tr1,X_te_1)\n",
    "#Best Hyperparameters:  {'C': 1, 'epsilon': 0.2, 'loss': 'epsilon_insensitive'}; Best Score:  0.34926931350937107\n",
    "\n",
    "# pred_SVM1 = param_selection({'loss': [\"epsilon_insensitive\", \"squared_epsilon_insensitive\"], 'epsilon': [0,0.01,0.05,0.1,0.2,0.5],  'C': [0.5,1,1.5]},LinearSVR(), X_tr_1, y_tr1, X_te_1)\n",
    "# Best Hyperparameters:  {'C': 1, 'epsilon': 0.2, 'loss': 'epsilon_insensitive'}; Best Score:  0.34948012695417774\n",
    "\n",
    "#BEST SCORE\n",
    "#pred_SVM1 = param_selection({'kernel': [\"linear\",\"poly\",\"rbf\", \"sigmoid\"], 'gamma':[\"scale\", \"auto\"], 'nu':[0.25,0.4,0.5,0.6,0.75,1], 'C': [0.5,1,1.5]}, NuSVR(), X_tr_1,y_tr1,X_te1)\n",
    "# Best Hyperparameters:  {'C': 1, 'gamma': 'scale', 'kernel': 'rbf', 'nu': 0.5}; Best Score:  0.37920360670150177\n",
    "pred_SVM1 = NuSVR(kernel=\"rbf\", gamma=\"scale\", nu=0.5,C=1).fit(X_tr_1,y_tr1.to_numpy().ravel()).predict(X_te_1)\n",
    "\n",
    "print(\"erreur test: \", r2_score(y_te1, pred_SVM1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 - Arbres de décision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modèle entrainé sur data0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters:  {'n_estimators': 1000}\n",
      "Best Score:  0.445343694590953\n",
      "erreur test:  0.5123931993006859\n"
     ]
    }
   ],
   "source": [
    "#param_grid = {\"learning_rate\":[0.01, 0.05,0.1,0.15]}\n",
    "#mod = GradientBoostingRegressor(random_state=10, loss=\"huber\", learning_rate= 0.15)\n",
    "#Best Score:  0.3803194835261793\n",
    "\n",
    "#param_grid = {loss:['squared_error', 'absolute_error', 'poisson', 'quantile'],\"learning_rate\":[0.15,0.1,0.075]}\n",
    "#mod = HistGradientBoostingRegressor(random_state=10, loss=\"poisson\", learning_rate= 0.15)\n",
    "#Best Score:  0.4212414218497866\n",
    "\n",
    "#param_grid = {'n_estimators': [1750,2000,2250]}\n",
    "#mod = BaggingRegressor(random_state=10, n_estimators=2250)\n",
    "# Best Hyperparameters:  {'n_estimators': 2250}; Best Score:  0.4322135469402454\n",
    "\n",
    "#BEST SCORE\n",
    "param_grid  = {'n_estimators': [900,1000,1100]}\n",
    "pred0 = param_selection(param_grid, ExtraTreesRegressor(random_state=50, max_samples=None), X_tr_0, y_tr0.to_numpy().ravel(), X_te_0)\n",
    "#Best Hyperparameters:  {'n_estimators': 1000}; Best Score:  0.445343694590953\n",
    "\n",
    "print(\"erreur test: \", r2_score(y_te0, pred0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modèle entrainé sur data1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters:  {'n_estimators': 2750}\n",
      "Best Score:  0.44143303715384785\n",
      "erreur test:  0.4469262599949042\n"
     ]
    }
   ],
   "source": [
    "#param_grid = {'n_estimators': [400,500]}\n",
    "#mod = BaggingRegressor(random_state=10)\n",
    "#Best Hyperparameters:  {'n_estimators': 500}; Best Score:  0.4415537219663822\n",
    "\n",
    "#BEST SCORE \n",
    "param_grid = {'n_estimators': [300,2750,100]}\n",
    "pred1 = param_selection(param_grid, ExtraTreesRegressor(random_state=50, max_samples=None), X_tr_1, y_tr1.to_numpy().ravel(), X_te_1)\n",
    "#Best Hyperparameters:  {'n_estimators': 2750}; Best Score:  0.44143303715384785\n",
    "\n",
    "print(\"erreur test: \", r2_score(y_te1, pred1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3- XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modèle entraîné sur data0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters:  {'n_estimators': 150}\n",
      "Best Score:  0.3286218368868324\n",
      "erreur test 0.3954196853517815\n"
     ]
    }
   ],
   "source": [
    "# preds = param_selection({'n_estimators': [95,100,105]}, xgb.XGBRegressor(seed = 2023, objective=\"reg:squarederror\"), X_tr_0, y_tr0, X_te_0)\n",
    "# Best Hyperparameters:  {'n_estimators': 100}; Best Score:  0.377947838196242\n",
    "\n",
    "#BEST SCORE\n",
    "preds0 = param_selection({'n_estimators': [100,150, 200, 250,300]}, xgb.XGBRFRegressor(random_state=50), X_tr_0, y_tr0, X_te_0)\n",
    "# Best Hyperparameters:  {'n_estimators': 200}; Best Score:  0.32986538836899715\n",
    "\n",
    "print(\"erreur test\",r2_score(y_te0, preds0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modèle entrainé sur data1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters:  {'n_estimators': 415}\n",
      "Best Score:  0.40799724415966443\n",
      "erreur test:  0.37489969193217565\n"
     ]
    }
   ],
   "source": [
    "# preds1 = param_selection({'objective':[\"reg:linear\", \"reg:squarederror\", \"reg:tweedie\", \"reg:huber\"],'n_estimators': [10,50,75,100,150,200]}, xgb.XGBRegressor(seed = 2023), X_tr_1, y_tr1, X_te_1)\n",
    "# Best Hyperparameters:  {'n_estimators': 10, 'objective': 'reg:linear'}; Best Score:  0.37838078901841915\n",
    "\n",
    "# preds1 = param_selection({'objective':[\"reg:linear\", \"reg:squarederror\"],'n_estimators': [5,10, 15,20]}, xgb.XGBRegressor(random_state = 2023), X_tr_1, y_tr1, X_te_1)\n",
    "# Best Hyperparameters:  {'n_estimators': 10, 'objective': 'reg:linear'}; Best Score:  0.37838078901841915\n",
    "\n",
    "#BEST SCORE\n",
    "preds1 = param_selection({'n_estimators': [415, 420, 425]}, xgb.XGBRFRegressor(seed=2023), X_tr_1, y_tr1, X_te_1)\n",
    "#Best Hyperparameters:  {'n_estimators': 420}; Best Score:  0.40799724415966443\n",
    "\n",
    "print(\"erreur test: \",r2_score(y_te1, preds1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datachallenge",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
